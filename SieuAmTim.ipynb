{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SieuAmTim.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quangtrung0220/SieuAmTim/blob/master/SieuAmTim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHqdaDyviNIb"
      },
      "source": [
        "### ***Import các thư viện cần thiết***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOR4tIyWxCtG"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS5NUyqriqjn"
      },
      "source": [
        "### ***Clone git để lấy dữ liệu ảnh train và test***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVMBQrJ-TkpR",
        "outputId": "22b6ed90-c3d0-4355-e8f5-4648bc422514"
      },
      "source": [
        "# Mount với drive để lấy datasets \n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Link git lấy dataset \n",
        "!git clone https://github.com/quangtrung0220/SieuAmTim.git\n",
        "traindir = \"/content/SieuAmTim/DATA_CHAMBER_2021/train\"\n",
        "testdir = \"/content/SieuAmTim/DATA_CHAMBER_2021/test\"\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SieuAmTim' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGn9YjaGi51f"
      },
      "source": [
        "### ***Lấy ra các nhãn - classes và chuẩn bị dữ liệu***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   Các nhãn: 2C, 3C, 4C\n",
        "*   Các loại kích cỡ ảnh train và test: 224x224, 32x32\n",
        "*   Preprocess: Cân bằng sáng: histogram, Lọc nhiễu: GaussianBlur\n",
        "*   Data Augmentation: RandomCrop, RandomHorizontalFlip, RandomVerticalFlip\n",
        "*   Đưa vào batch để tận dụng khả năng xử lý song song của GPU\n",
        "*   Folder train và test lần lượt trong: traindir và testdir\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-Ekbuc8UWLr"
      },
      "source": [
        "# Hàm lấy ra các classes\n",
        "def get_clases():\n",
        "  classes = ['2C', '3C', '4C']\n",
        "  return classes\n",
        "\n",
        "TrainTest = namedtuple('TrainTest', ['train', 'test'])\n",
        "\n",
        "# Hàm chuẩn bị dữ liệu\n",
        "def prepare_data():\n",
        "\n",
        "  # Preprocess\n",
        "  gaussianBlur = transforms.GaussianBlur(3)\n",
        "  histogramEqualize = transforms.RandomEqualize(p=0.5)\n",
        "\n",
        "  # #Normalize\n",
        "  # norm = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "  #      std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  #Augmentation for all\n",
        "  horizontal = transforms.RandomHorizontalFlip()\n",
        "  vertical = transforms.RandomVerticalFlip()\n",
        "\n",
        "  #Image 224x224\n",
        "  resize_224 = transforms.Resize((224,224))\n",
        "  crop_224 = transforms.RandomCrop(224, padding=4)\n",
        "  \n",
        "  #Image 32x32\n",
        "  resize_32 = transforms.Resize((32,32))\n",
        "  crop_32 = transforms.RandomCrop(32, padding=4)\n",
        "\n",
        "  #To tensor\n",
        "  tensor = transforms.ToTensor()\n",
        "\n",
        "  #Raw image\n",
        "  transform_train_raw224 = transforms.Compose([resize_224, tensor])\n",
        "  transform_train_raw232 = transforms.Compose([resize_32, tensor])\n",
        "\n",
        "  #Augmentation image\n",
        "  transform_train_aug224 = transforms.Compose([resize_224, crop_224, horizontal, vertical, tensor])\n",
        "  transform_train_aug32 = transforms.Compose([resize_32, crop_32, horizontal, vertical, tensor])\n",
        "\n",
        "  #Preprocess image\n",
        "  transform_train_pre224 = transforms.Compose([resize_224, gaussianBlur, histogramEqualize, tensor])\n",
        "  transform_train_pre32 = transforms.Compose([resize_32, gaussianBlur, histogramEqualize, tensor])\n",
        "\n",
        "  #Test image\n",
        "  transform_test_224 = transforms.Compose([resize_224, tensor])\n",
        "  transform_test_32 = transforms.Compose([resize_32, tensor])\n",
        "\n",
        "  # transform_test = transforms.Compose([\n",
        "  #   transforms.Resize((224,224)),\n",
        "  #   # transforms.Resize((32,32)),\n",
        "\n",
        "  #   # Preprocess\n",
        "  #   transforms.GaussianBlur(3),\n",
        "  #   transforms.RandomEqualize(p=0.5),\n",
        "\n",
        "  #   transforms.ToTensor(),\n",
        "  #   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "  #      std=[0.229, 0.224, 0.225])\n",
        "  # ])\n",
        "  trainset = torchvision.datasets.ImageFolder(root=traindir, transform=transform_train_aug224)\n",
        "  testset = torchvision.datasets.ImageFolder(root=testdir, transform=transform_test_224)\n",
        "  return TrainTest(train=trainset, test=testset)\n",
        "\n",
        "# Hàm load dữ liệu\n",
        "def prepare_loader(datasets):\n",
        "  trainloader = DataLoader(dataset=datasets.train, batch_size=35, shuffle=True, num_workers=4)\n",
        "  testloader = DataLoader(dataset=datasets.test, batch_size=35, shuffle=False, num_workers=4)\n",
        "  return TrainTest(train=trainloader, test=testloader)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYQS8gvZqB6F"
      },
      "source": [
        "### ***Hàm train và test***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   BatchSize = 35\n",
        "*   Report sau mỗi 20 epoch \n",
        "*   Train: cho đi qua model và tính lỗi bằng loss_func sau đó cập nhật tham số mới\n",
        "*   Test: khai báo chế độ eval(đánh giá - evaluate) và trả về nhãn dự đoán và nhãn thực tế\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5aaQjZ6q7tM"
      },
      "source": [
        "# Data = prepare_data()\n",
        "# img, label = Data.train[3000]\n",
        "# print(label)\n",
        "# plt.show(img)\n",
        "# print(img)\n",
        "# s = Data.train.imgs[0]\n",
        "# print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBdgazuRRB3K"
      },
      "source": [
        "# Hàm train \n",
        "def train_epoch(epoch, model, loader, loss_func, optimizer, device):\n",
        "  true_result = []\n",
        "  pred_result = []\n",
        "\n",
        "  # gọi hàm train để model biết mình phải cần huấn luyện \n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  reporting_steps = 20\n",
        "  for i, (images, labels) in enumerate(loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    outputs = model(images)\n",
        "    true_result += list(labels.cpu().numpy())\n",
        "    _, predicted = torch.max(outputs, dim=1)\n",
        "    pred_result += list(predicted.cpu().numpy())\n",
        "    # Tính lỗi \n",
        "    loss = loss_func(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % reporting_steps == reporting_steps-1:\n",
        "      print(f\"Epoch {epoch} step {i} ave_loss {running_loss/reporting_steps:.4f}\")\n",
        "      running_loss = 0.0\n",
        "  # return pred_result, true_result\n",
        "\n",
        "# hàm test \n",
        "def test_epoch(epoch, model, loader, device):\n",
        "  ytrue = []\n",
        "  ypred = []\n",
        "  # Gọi hàm eval để model biết đang ở chế độ test \n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    \n",
        "    for i, (images, labels) in enumerate(loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "      ytrue += list(labels.cpu().numpy())\n",
        "      ypred += list(predicted.cpu().numpy())\n",
        "\n",
        "  return ypred, ytrue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e21jIMWPrUsA"
      },
      "source": [
        "### ***Hàm main***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   model_in = 'vgg16', 'resnet50' hoặc 'desnet': tên mạng Cnn đùng để huấn luyện\n",
        "*   PATH: đường dẫn lưu trọng số sau khi huấn luyện\n",
        "*   Đặc trưng ra = 3 vì có 3 lớp 2C, 3C, 4C\n",
        "*   Hàm lỗi được sử dụng: Cross-Entropy\n",
        "*   Hàm tối ưu đước sử dụng: SGD - Stochastic Gradient Descent\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA-A25O0RLTG",
        "outputId": "bb404184-909c-40c2-f33e-dbf521ec8f88"
      },
      "source": [
        "def main(PATH='./model.pth', model_in=''):\n",
        "  classes = get_clases()\n",
        "  datasets = prepare_data()\n",
        "  # img, label = datasets.train[0]\n",
        "  # plt.imshow(img)\n",
        "  # print(classes[label], img.size)\n",
        "  # print('train', len(datasets.train), 'test', len(datasets.test))\n",
        "  \n",
        "  loaders = prepare_loader(datasets)\n",
        "\n",
        "  # images, labels = iter(loaders.train).next()\n",
        "  # print(images.shape, labels.shape)\n",
        "\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # model = ResNet50().to(device)\n",
        "  # model = VGG16().to(device)\n",
        "  # images, labels = iter(loaders.train).next()\n",
        "  # outputs = model(images)\n",
        "  # print(outputs.shape)\n",
        "  # print(outputs[0])\n",
        "  # _, predicted = torch.max(outputs, dim=1)\n",
        "  # print(predicted)\n",
        "  # imshow(images, labels, predicted, classes)\n",
        "\n",
        "# Load model \n",
        "  if model_in == 'vgg16':\n",
        "    model = torchvision.models.vgg16()\n",
        "    model.classifier[6] = torch.nn.modules.linear.Linear(in_features=4096, out_features=3, bias=True)\n",
        "    # if torch.cuda.is_available():\n",
        "    model.to(device=device)\n",
        "  elif model_in == 'resnet50':\n",
        "    model = torchvision.models.resnet50(pretrained=False, progress=False)\n",
        "    model.fc = torch.nn.modules.linear.Linear(in_features=2048, out_features=3, bias=True)  \n",
        "    # if torch.cuda.is_available():\n",
        "    model.to(device=device)\n",
        "  elif model_in == 'desnet':\n",
        "    model = torchvision.models.densenet121(pretrained=False, progress=False)\n",
        "    model.classifier = torch.nn.modules.linear.Linear(in_features=1024, out_features=3, bias=True)\n",
        "    # if torch.cuda.is_available():\n",
        "    model.to(device=device)\n",
        "  else:\n",
        "    pass  \n",
        "\n",
        "  # array_accuracy = []\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "  for epoch in range(10):\n",
        "    print(f'Epoch {epoch} report: ')\n",
        "    \n",
        "    # pred_result, true_result = \n",
        "    train_epoch(epoch, model, loaders.train, loss_func, optimizer, device)\n",
        "    # print('Train report: ')\n",
        "    # print(classification_report(true_result, pred_result, target_names=classes))\n",
        "\n",
        "    print('Test report: ')\n",
        "    ypred, ytrue = test_epoch(epoch, model, loaders.test, device)\n",
        "\n",
        "    # accuracy = (ytrue_test==ypred_test).sum() / len(ytrue_test)\n",
        "    # array_accuracy.append(accuracy)\n",
        "\n",
        "    print(classification_report(ytrue, ypred, target_names=classes))\n",
        "\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "\n",
        "  return model\n",
        "\n",
        "model = main(PATH=\"./desnet121.pth\", model_in='desnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 report: \n",
            "Epoch 0 step 19 ave_loss 1.0896\n",
            "Epoch 0 step 39 ave_loss 0.9752\n",
            "Epoch 0 step 59 ave_loss 0.9268\n",
            "Epoch 0 step 79 ave_loss 0.8415\n",
            "Epoch 0 step 99 ave_loss 0.8537\n",
            "Epoch 0 step 119 ave_loss 0.9380\n",
            "Epoch 0 step 139 ave_loss 0.7161\n",
            "Epoch 0 step 159 ave_loss 0.5320\n",
            "Epoch 0 step 179 ave_loss 0.4864\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.45      0.82      0.58       409\n",
            "          3C       0.23      0.51      0.32       367\n",
            "          4C       1.00      0.05      0.10       831\n",
            "\n",
            "    accuracy                           0.35      1607\n",
            "   macro avg       0.56      0.46      0.33      1607\n",
            "weighted avg       0.68      0.35      0.27      1607\n",
            "\n",
            "Epoch 1 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 step 19 ave_loss 0.4579\n",
            "Epoch 1 step 39 ave_loss 0.4649\n",
            "Epoch 1 step 59 ave_loss 0.4700\n",
            "Epoch 1 step 79 ave_loss 0.3703\n",
            "Epoch 1 step 99 ave_loss 0.2924\n",
            "Epoch 1 step 119 ave_loss 0.2788\n",
            "Epoch 1 step 139 ave_loss 0.2997\n",
            "Epoch 1 step 159 ave_loss 0.2077\n",
            "Epoch 1 step 179 ave_loss 0.2292\n",
            "Test report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.26      1.00      0.41       409\n",
            "          3C       0.00      0.00      0.00       367\n",
            "          4C       0.00      0.00      0.00       831\n",
            "\n",
            "    accuracy                           0.25      1607\n",
            "   macro avg       0.09      0.33      0.14      1607\n",
            "weighted avg       0.07      0.25      0.10      1607\n",
            "\n",
            "Epoch 2 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 step 19 ave_loss 0.2014\n",
            "Epoch 2 step 39 ave_loss 0.2489\n",
            "Epoch 2 step 59 ave_loss 0.2632\n",
            "Epoch 2 step 79 ave_loss 0.1893\n",
            "Epoch 2 step 99 ave_loss 0.1661\n",
            "Epoch 2 step 119 ave_loss 0.0823\n",
            "Epoch 2 step 139 ave_loss 0.1071\n",
            "Epoch 2 step 159 ave_loss 0.1331\n",
            "Epoch 2 step 179 ave_loss 0.1400\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.74      0.68      0.71       409\n",
            "          3C       0.51      0.86      0.64       367\n",
            "          4C       0.96      0.71      0.82       831\n",
            "\n",
            "    accuracy                           0.74      1607\n",
            "   macro avg       0.74      0.75      0.72      1607\n",
            "weighted avg       0.80      0.74      0.75      1607\n",
            "\n",
            "Epoch 3 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 step 19 ave_loss 0.1091\n",
            "Epoch 3 step 39 ave_loss 0.0765\n",
            "Epoch 3 step 59 ave_loss 0.0593\n",
            "Epoch 3 step 79 ave_loss 0.0525\n",
            "Epoch 3 step 99 ave_loss 0.1291\n",
            "Epoch 3 step 119 ave_loss 0.1788\n",
            "Epoch 3 step 139 ave_loss 0.1741\n",
            "Epoch 3 step 159 ave_loss 0.1154\n",
            "Epoch 3 step 179 ave_loss 0.0684\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.85      0.63      0.72       409\n",
            "          3C       0.52      0.93      0.67       367\n",
            "          4C       0.99      0.77      0.87       831\n",
            "\n",
            "    accuracy                           0.77      1607\n",
            "   macro avg       0.79      0.78      0.75      1607\n",
            "weighted avg       0.85      0.77      0.78      1607\n",
            "\n",
            "Epoch 4 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 step 19 ave_loss 0.0494\n",
            "Epoch 4 step 39 ave_loss 0.0674\n",
            "Epoch 4 step 59 ave_loss 0.0584\n",
            "Epoch 4 step 79 ave_loss 0.0607\n",
            "Epoch 4 step 99 ave_loss 0.0410\n",
            "Epoch 4 step 119 ave_loss 0.0238\n",
            "Epoch 4 step 139 ave_loss 0.0207\n",
            "Epoch 4 step 159 ave_loss 0.0333\n",
            "Epoch 4 step 179 ave_loss 0.0323\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.57      0.99      0.73       409\n",
            "          3C       0.94      0.63      0.76       367\n",
            "          4C       0.96      0.75      0.84       831\n",
            "\n",
            "    accuracy                           0.79      1607\n",
            "   macro avg       0.82      0.79      0.78      1607\n",
            "weighted avg       0.86      0.79      0.79      1607\n",
            "\n",
            "Epoch 5 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 step 19 ave_loss 0.0493\n",
            "Epoch 5 step 39 ave_loss 0.0291\n",
            "Epoch 5 step 59 ave_loss 0.0259\n",
            "Epoch 5 step 79 ave_loss 0.0343\n",
            "Epoch 5 step 99 ave_loss 0.0253\n",
            "Epoch 5 step 119 ave_loss 0.0672\n",
            "Epoch 5 step 139 ave_loss 0.0730\n",
            "Epoch 5 step 159 ave_loss 0.0564\n",
            "Epoch 5 step 179 ave_loss 0.0758\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.57      0.93      0.71       409\n",
            "          3C       0.92      0.67      0.78       367\n",
            "          4C       0.90      0.72      0.80       831\n",
            "\n",
            "    accuracy                           0.77      1607\n",
            "   macro avg       0.80      0.78      0.76      1607\n",
            "weighted avg       0.82      0.77      0.77      1607\n",
            "\n",
            "Epoch 6 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 step 19 ave_loss 0.1052\n",
            "Epoch 6 step 39 ave_loss 0.0767\n",
            "Epoch 6 step 59 ave_loss 0.0633\n",
            "Epoch 6 step 79 ave_loss 0.0258\n",
            "Epoch 6 step 99 ave_loss 0.0291\n",
            "Epoch 6 step 119 ave_loss 0.0401\n",
            "Epoch 6 step 139 ave_loss 0.0321\n",
            "Epoch 6 step 159 ave_loss 0.0356\n",
            "Epoch 6 step 179 ave_loss 0.0259\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.56      0.81      0.66       409\n",
            "          3C       0.69      0.89      0.77       367\n",
            "          4C       0.96      0.62      0.75       831\n",
            "\n",
            "    accuracy                           0.73      1607\n",
            "   macro avg       0.73      0.77      0.73      1607\n",
            "weighted avg       0.79      0.73      0.73      1607\n",
            "\n",
            "Epoch 7 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 step 19 ave_loss 0.0171\n",
            "Epoch 7 step 39 ave_loss 0.0054\n",
            "Epoch 7 step 59 ave_loss 0.0057\n",
            "Epoch 7 step 79 ave_loss 0.0208\n",
            "Epoch 7 step 99 ave_loss 0.0143\n",
            "Epoch 7 step 119 ave_loss 0.0157\n",
            "Epoch 7 step 139 ave_loss 0.0180\n",
            "Epoch 7 step 159 ave_loss 0.0257\n",
            "Epoch 7 step 179 ave_loss 0.0165\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.84      0.64      0.73       409\n",
            "          3C       0.56      0.87      0.68       367\n",
            "          4C       0.94      0.81      0.87       831\n",
            "\n",
            "    accuracy                           0.78      1607\n",
            "   macro avg       0.78      0.78      0.76      1607\n",
            "weighted avg       0.83      0.78      0.79      1607\n",
            "\n",
            "Epoch 8 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 step 19 ave_loss 0.0233\n",
            "Epoch 8 step 39 ave_loss 0.0193\n",
            "Epoch 8 step 59 ave_loss 0.0138\n",
            "Epoch 8 step 79 ave_loss 0.0170\n",
            "Epoch 8 step 99 ave_loss 0.0174\n",
            "Epoch 8 step 119 ave_loss 0.0214\n",
            "Epoch 8 step 139 ave_loss 0.0111\n",
            "Epoch 8 step 159 ave_loss 0.0200\n",
            "Epoch 8 step 179 ave_loss 0.0117\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.56      0.88      0.69       409\n",
            "          3C       0.80      0.90      0.84       367\n",
            "          4C       1.00      0.67      0.80       831\n",
            "\n",
            "    accuracy                           0.77      1607\n",
            "   macro avg       0.79      0.81      0.78      1607\n",
            "weighted avg       0.84      0.77      0.78      1607\n",
            "\n",
            "Epoch 9 report: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 step 19 ave_loss 0.0091\n",
            "Epoch 9 step 39 ave_loss 0.0134\n",
            "Epoch 9 step 59 ave_loss 0.0054\n",
            "Epoch 9 step 79 ave_loss 0.0087\n",
            "Epoch 9 step 99 ave_loss 0.0100\n",
            "Epoch 9 step 119 ave_loss 0.0221\n",
            "Epoch 9 step 139 ave_loss 0.0184\n",
            "Epoch 9 step 159 ave_loss 0.0242\n",
            "Epoch 9 step 179 ave_loss 0.0101\n",
            "Test report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.57      1.00      0.72       409\n",
            "          3C       0.93      0.75      0.83       367\n",
            "          4C       0.94      0.67      0.78       831\n",
            "\n",
            "    accuracy                           0.77      1607\n",
            "   macro avg       0.81      0.81      0.78      1607\n",
            "weighted avg       0.84      0.77      0.78      1607\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zL2AY3Lumw-"
      },
      "source": [
        "### ***Test Code ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM2cXz5GloSc",
        "outputId": "9ed8825c-0120-4f87-8f13-dd3f9c4a6dd3"
      },
      "source": [
        "# Data = prepare_data()\n",
        "# frame_id_list = []\n",
        "# for i, image in enumerate(Data.test.imgs):\n",
        "#     id = image[0].split(\"/\")[-1].split(\"_\")[0]\n",
        "#     frame_id_list.append(id)\n",
        "# print(frame_id_list) "
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '158', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '165', '168', '168', '168', '168', '168', '168', '168', '168', '168', '168', '168', '168', '168', '168', '168', '168', '168', '168', '168', '168', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '169', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '171', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '176', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '177', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '178', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '181', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '183', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '191', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '192', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '157', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '159', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '161', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '162', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '166', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '174', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '175', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '179', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '185', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '186', '189', '189', '189', '189', '189', '189', '189', '189', '189', '189', '189', '189', '189', '189', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '190', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '194', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '160', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '163', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '164', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '167', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '170', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '172', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '173', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '182', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '184', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '187', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '188', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '193', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195', '195']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNsoCpFnr051",
        "outputId": "32c29fd3-15c6-4522-e009-a1cd3f234239"
      },
      "source": [
        "# video_list = [] \n",
        "# for frame in frame_id_list:\n",
        "#   if (frame in video_list) == False:\n",
        "#     video_list.append(frame)\n",
        "#   else:\n",
        "#        pass\n",
        "# print(video_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['158', '165', '168', '169', '171', '176', '177', '178', '181', '183', '191', '192', '157', '159', '161', '162', '166', '174', '175', '179', '185', '186', '189', '190', '194', '160', '163', '164', '167', '170', '172', '173', '180', '182', '184', '187', '188', '193', '195']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRz_EdFDtnHX",
        "outputId": "ff538df5-b428-4d0b-cc84-4bacbaaa57a1"
      },
      "source": [
        "a = [1,2,3,2,3,1,1,1,1,1,3,2]\n",
        "values_arr, counts_arr = np.unique(a, return_counts=True)\n",
        "print(values_arr)\n",
        "print(counts_arr)\n",
        "result = np.where(counts_arr == np.max(counts_arr))\n",
        "print(values_arr[result])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3]\n",
            "[6 3 3]\n",
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BNTzYfSh20E"
      },
      "source": [
        "### ***Phân lớp với Video***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   Với kết quả huấn luyện bên trên thì cho ảnh qua ta sẽ có kết quả dự đoán lớp của ảnh input mới\n",
        "*   1 video sẽ có nhiều frame nên kết quả phân lớp video sẽ là số nhãn xuất hiện nhiều nhất trong số các frame của video\n",
        "*   Giả sử video A có 90% nhãn được dự đoán là lớp 2C => video thuộc lớp 2C2C\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyqv-M1dhrAI"
      },
      "source": [
        "def video_classify(model=None, testdir=None, device='cuda'):\n",
        "\n",
        "  # check testsets\n",
        "  if testdir == None:\n",
        "    testdir = torchvision.datasets.ImageFolder(root=testdir, \n",
        "                                               transform=ransforms.Compose([resize_224, transforms.ToTensor()]))\n",
        "\n",
        "  video = namedtuple('video', ['id', \"label_true\", 'label_pred'])\n",
        "\n",
        "  # get id list\n",
        "  frame_id_list = []\n",
        "  for i, image in enumerate(testdir.imgs):\n",
        "    id = image[0].split(\"/\")[-1].split(\"_\")[0]\n",
        "    frame_id_list.append(id)\n",
        "\n",
        "  # get list video\n",
        "  video_list = np.unique(frame_id_list, return_counts=False)\n",
        "\n",
        "  ytrue = []\n",
        "  ypred = []\n",
        "  model.to(device)\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    \n",
        "    for images, labels in testdir:\n",
        "      images = images.unsqueeze(0).to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, dim=1)\n",
        "      ytrue.append(labels)\n",
        "      ypred += list(predicted.cpu().numpy())\n",
        "\n",
        "  # if id in frame_id_list = id in video_list: add yred and ytrue to pred_vid and true_vid\n",
        "  outputs = []\n",
        "  ytrue_video = []\n",
        "  ypred_video = []\n",
        "  for id in video_list:\n",
        "    true_vid = []\n",
        "    pred_vid = []\n",
        "    for index, img in enumerate(frame_id_list):\n",
        "      if img == id:\n",
        "        pred_vid.append(ypred[index])\n",
        "        true_vid.append(ytrue[index])\n",
        "\n",
        "    # classification\n",
        "    value_true, count_true = np.unique(true_vid, return_counts=True)\n",
        "    label_true = value_true[np.where(count_true == np.max(count_true))]\n",
        "\n",
        "    value_pred, count_pred = np.unique(pred_vid, return_counts=True)\n",
        "    label_pred = value_pred[np.where(count_pred == np.max(count_pred))]\n",
        "\n",
        "    print(\"id:\", id, \", true:\", label_true, \", pred:\",label_pred)\n",
        "    ytrue_video.append(label_true)\n",
        "    ypred_video.append(label_pred)\n",
        "    outputs.append(video(id=id, label_true=label_true, label_pred=label_pred))\n",
        "\n",
        "  classes = get_clases()\n",
        "  print(classification_report(ytrue_video, ypred_video, target_names=classes))\n",
        "  return outputs\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2m6eoMbyxum",
        "outputId": "c90b383d-a57d-4dbd-cbca-987391b3741d"
      },
      "source": [
        "Data = prepare_data()\n",
        "testset = Data.test\n",
        "video_classify(model=model, testdir=testset, device='cuda')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id: 157 , true: [1] , pred: [1]\n",
            "id: 158 , true: [0] , pred: [0]\n",
            "id: 159 , true: [1] , pred: [1]\n",
            "id: 160 , true: [2] , pred: [2]\n",
            "id: 161 , true: [1] , pred: [1]\n",
            "id: 162 , true: [1] , pred: [1]\n",
            "id: 163 , true: [2] , pred: [2]\n",
            "id: 164 , true: [2] , pred: [2]\n",
            "id: 165 , true: [0] , pred: [0]\n",
            "id: 166 , true: [1] , pred: [1]\n",
            "id: 167 , true: [2] , pred: [2]\n",
            "id: 168 , true: [0] , pred: [0]\n",
            "id: 169 , true: [0] , pred: [0]\n",
            "id: 170 , true: [2] , pred: [0]\n",
            "id: 171 , true: [0] , pred: [0]\n",
            "id: 172 , true: [2] , pred: [2]\n",
            "id: 173 , true: [2] , pred: [2]\n",
            "id: 174 , true: [1] , pred: [0]\n",
            "id: 175 , true: [1] , pred: [1]\n",
            "id: 176 , true: [0] , pred: [0]\n",
            "id: 177 , true: [0] , pred: [0]\n",
            "id: 178 , true: [0] , pred: [0]\n",
            "id: 179 , true: [1] , pred: [0]\n",
            "id: 180 , true: [2] , pred: [2]\n",
            "id: 181 , true: [0] , pred: [0]\n",
            "id: 182 , true: [2] , pred: [2]\n",
            "id: 183 , true: [0] , pred: [0]\n",
            "id: 184 , true: [2] , pred: [2]\n",
            "id: 185 , true: [1] , pred: [1]\n",
            "id: 186 , true: [1] , pred: [1]\n",
            "id: 187 , true: [2] , pred: [0]\n",
            "id: 188 , true: [2] , pred: [0]\n",
            "id: 189 , true: [1] , pred: [2]\n",
            "id: 190 , true: [1] , pred: [1]\n",
            "id: 191 , true: [0] , pred: [0]\n",
            "id: 192 , true: [0] , pred: [0]\n",
            "id: 193 , true: [2] , pred: [0]\n",
            "id: 194 , true: [1] , pred: [1]\n",
            "id: 195 , true: [2] , pred: [0]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.63      1.00      0.77        12\n",
            "          3C       1.00      0.77      0.87        13\n",
            "          4C       0.90      0.64      0.75        14\n",
            "\n",
            "    accuracy                           0.79        39\n",
            "   macro avg       0.84      0.80      0.80        39\n",
            "weighted avg       0.85      0.79      0.80        39\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[video(id='157', label_true=array([1]), label_pred=array([1])),\n",
              " video(id='158', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='159', label_true=array([1]), label_pred=array([1])),\n",
              " video(id='160', label_true=array([2]), label_pred=array([2])),\n",
              " video(id='161', label_true=array([1]), label_pred=array([1])),\n",
              " video(id='162', label_true=array([1]), label_pred=array([1])),\n",
              " video(id='163', label_true=array([2]), label_pred=array([2])),\n",
              " video(id='164', label_true=array([2]), label_pred=array([2])),\n",
              " video(id='165', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='166', label_true=array([1]), label_pred=array([1])),\n",
              " video(id='167', label_true=array([2]), label_pred=array([2])),\n",
              " video(id='168', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='169', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='170', label_true=array([2]), label_pred=array([0])),\n",
              " video(id='171', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='172', label_true=array([2]), label_pred=array([2])),\n",
              " video(id='173', label_true=array([2]), label_pred=array([2])),\n",
              " video(id='174', label_true=array([1]), label_pred=array([0])),\n",
              " video(id='175', label_true=array([1]), label_pred=array([1])),\n",
              " video(id='176', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='177', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='178', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='179', label_true=array([1]), label_pred=array([0])),\n",
              " video(id='180', label_true=array([2]), label_pred=array([2])),\n",
              " video(id='181', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='182', label_true=array([2]), label_pred=array([2])),\n",
              " video(id='183', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='184', label_true=array([2]), label_pred=array([2])),\n",
              " video(id='185', label_true=array([1]), label_pred=array([1])),\n",
              " video(id='186', label_true=array([1]), label_pred=array([1])),\n",
              " video(id='187', label_true=array([2]), label_pred=array([0])),\n",
              " video(id='188', label_true=array([2]), label_pred=array([0])),\n",
              " video(id='189', label_true=array([1]), label_pred=array([2])),\n",
              " video(id='190', label_true=array([1]), label_pred=array([1])),\n",
              " video(id='191', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='192', label_true=array([0]), label_pred=array([0])),\n",
              " video(id='193', label_true=array([2]), label_pred=array([0])),\n",
              " video(id='194', label_true=array([1]), label_pred=array([1])),\n",
              " video(id='195', label_true=array([2]), label_pred=array([0]))]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    }
  ]
}